{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "CUiLU2CT-tb6"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "5Qqh9AfS-8ai"
   },
   "outputs": [],
   "source": [
    "ko_1_size = 15\n",
    "wt_1_size = 15\n",
    "ko_2_size = 41\n",
    "wt_2_size = 41\n",
    "ko_3_size = 16\n",
    "wt_3_size = 18\n",
    "\n",
    "ko_1_file_prefix = '1KO'\n",
    "wt_1_file_prefix = '1WT'\n",
    "ko_2_file_prefix = '2KO'\n",
    "wt_2_file_prefix = '2WT'\n",
    "ko_3_file_prefix = '3KO'\n",
    "wt_3_file_prefix = '3WT'\n",
    "\n",
    "ko_mice_symbol = 0\n",
    "wt_mice_symbol = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_trail_num = 1280\n",
    "# model params\n",
    "learning_rate = 0.0001\n",
    "reduce_lr_factor = 0.5\n",
    "reduce_lr_patience = 5\n",
    "min_learning_rate = 0.00001\n",
    "early_stopping_patience = 10\n",
    "batch_size = 32\n",
    "epochs = 13\n",
    "validation_split = 0.2\n",
    "metric_cut_percent = 0.2\n",
    "metric_lower_cut_percent = 0.2\n",
    "metric_upper_cut_percent = 0.2\n",
    "noise_deviation = 0.00005\n",
    "\n",
    "# data augmentation params\n",
    "data_augmentation_factor = 3\n",
    "step_size = 40\n",
    "test_data_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-8E4RgKo-8d1",
    "outputId": "49357805-0bbf-4bd7-eb1b-2ae407a7d135"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KO data dir exists: True\n",
      "WT data dir exists: True\n"
     ]
    }
   ],
   "source": [
    "ko_directory = r'/data/KO/'\n",
    "wt_directory = r'/data/WT/'\n",
    "print('KO data dir exists: ' + str(os.path.exists(ko_directory)))\n",
    "print('WT data dir exists: ' + str(os.path.exists(wt_directory)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0wkZDdks-8gD"
   },
   "outputs": [],
   "source": [
    "def read_x_data(file_dir, data_size, file_prefix):\n",
    "    train_x = []\n",
    "    train_record = []\n",
    "    for i in range(data_size):\n",
    "        train_x.append(pd.read_excel(os.path.join(file_dir, file_prefix + str(i+1) + '.xlsx'), dtype='int16', header=None, sheet_name='Sheet1'))\n",
    "        train_record.append(pd.read_excel(os.path.join(file_dir, file_prefix + str(i+1) + '.xlsx'), dtype='int16', header=None, sheet_name='Sheet2'))\n",
    "\n",
    "    return train_x, train_record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def downsample(data, time_factor=2):\n",
    "#     \"\"\"\n",
    "#     input shape： (n_samples, trails, timesteps, features)\n",
    "#     output shape： (n_samples, trails, timesteps//factor, features)\n",
    "#     \"\"\"\n",
    "#     return data[:, ::time_factor, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_data_pre_process(behavior_data_list, record_data_list, with_noise):\n",
    "    data_size = len(behavior_data_list)\n",
    "    pre_processed_data = []\n",
    "    \n",
    "    for i in range(data_size):\n",
    "        behavior_data = np.expand_dims(behavior_data_list[i].values[:, 200:1000], axis=-1)\n",
    "        record_data = record_data_list[i]\n",
    "        operation_data = record_data.loc[0]\n",
    "        odor1_data = np.zeros((behavior_data.shape[0], behavior_data.shape[1], 1))\n",
    "        odor2_data = np.zeros((behavior_data.shape[0], behavior_data.shape[1], 1))\n",
    "        reward_data = np.zeros((behavior_data.shape[0], behavior_data.shape[1], 1))\n",
    "        cur_trail_count_data = np.zeros((behavior_data.shape[0], behavior_data.shape[1], 1))\n",
    "        trail_count_data = np.zeros((behavior_data.shape[0], behavior_data.shape[1], 1))\n",
    "        \n",
    "        sum_trail_count = operation_data.shape[0]\n",
    "        for j in range(sum_trail_count):\n",
    "            lick_data = behavior_data[j]\n",
    "            trail_result = -1\n",
    "            odor = -1\n",
    "\n",
    "            if(operation_data[j]==1):\n",
    "                odor = 2\n",
    "                odor2_data[j, 0:100, :] = 1\n",
    "\n",
    "                # search lick index\n",
    "                lick_index = -1\n",
    "                for k in range(300,500):\n",
    "                    if(lick_data[k][0] == 1):\n",
    "                        lick_index = k\n",
    "                        break\n",
    "\n",
    "                if(lick_index > 0):\n",
    "                    reward_data[j, (lick_index +1):(lick_index + 51), :] = 1\n",
    "                    trail_result = 1\n",
    "                else:\n",
    "                    trail_result = 2\n",
    "            else:\n",
    "                odor = 1\n",
    "                odor1_data[j, 0:100, :] = 1\n",
    "\n",
    "                # search lick index\n",
    "                lick_index = -1\n",
    "                for k in range(0,600):\n",
    "                    if(lick_data[k]==1):\n",
    "                        lick_index = k\n",
    "                        break\n",
    "\n",
    "                # append result data\n",
    "                if(lick_index>0):\n",
    "                    trail_result = 3\n",
    "                else:\n",
    "                    trail_result = 4\n",
    "\n",
    "            cur_trail_count_data[j, :, :] = (j + 1) / sum_trail_count\n",
    "        \n",
    "        trail_count_data[:sum_trail_count, :, :] = sum_trail_count / 1850.0\n",
    "        x_data = np.concatenate((behavior_data, odor1_data, odor1_data, reward_data, cur_trail_count_data, trail_count_data), axis=2)\n",
    "        \n",
    "        if with_noise:\n",
    "            noise = np.random.normal(0, noise_deviation, x_data.shape)\n",
    "            x_data += noise\n",
    "\n",
    "        pre_processed_data.append(x_data)\n",
    "        \n",
    "    return pre_processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessed_x_data(file_dir, data_size, file_prefix, with_noise):\n",
    "    lick_data, record_data = read_x_data(file_dir, data_size, file_prefix)\n",
    "    pre_processed_data = train_data_pre_process(lick_data, record_data, with_noise)\n",
    "    return pre_processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_x_data(x_data_list, max_trail_count):\n",
    "    padded_x_data_list = []\n",
    "    for x_data in x_data_list:\n",
    "        if max_trail_count > len(x_data):\n",
    "            padding_size = max_trail_count - len(x_data)\n",
    "            padded = np.zeros((padding_size, x_data.shape[1], x_data.shape[2]))\n",
    "            padded_x_data = np.concatenate((x_data, padded), axis=0)\n",
    "            padded_x_data_list.append(padded_x_data)\n",
    "        else:\n",
    "            padded_x_data_list.append(x_data[:max_trail_count, :, :]) \n",
    "            \n",
    "    return padded_x_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_x_y(ko_x_data, wt_x_data, split=0.2):\n",
    "    ko_data_length = len(ko_x_data)\n",
    "    wt_data_length = len(wt_x_data)\n",
    "    ko_y_data = np.zeros(ko_data_length)\n",
    "    ko_y_data[:] = ko_mice_symbol\n",
    "    wt_y_data = np.zeros(wt_data_length)\n",
    "    wt_y_data[:] = wt_mice_symbol\n",
    "    \n",
    "    ko_split = int(ko_data_length * (1 - split))\n",
    "    wt_split = int(wt_data_length * (1 - split))\n",
    "    \n",
    "    ko_x_data = np.array(ko_x_data)\n",
    "    wt_x_data = np.array(wt_x_data)\n",
    "    \n",
    "    train_x = np.concatenate([ko_x_data[:ko_split], wt_x_data[:wt_split]])\n",
    "    train_y = np.concatenate([ko_y_data[:ko_split], wt_y_data[:wt_split]])\n",
    "    \n",
    "    val_x = np.concatenate([ko_x_data[ko_split:], wt_x_data[wt_split:]])\n",
    "    val_y = np.concatenate([ko_y_data[ko_split:], wt_y_data[wt_split:]])\n",
    "    \n",
    "    x_data = np.concatenate([train_x, val_x])\n",
    "    y_data = np.concatenate([train_y, val_y])\n",
    "    \n",
    "    return x_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_max_trail_count(data_array):\n",
    "    return max([data.shape[0] for data in data_array]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sliding_window_size(x_data, augmentation_factor, step_size):\n",
    "    trail_nums = [x.shape[0] for x in x_data]\n",
    "    max_trail_num = max(trail_nums)\n",
    "    if augmentation_factor <= 1:\n",
    "        return max_trail_num\n",
    "    return max_trail_num - (augmentation_factor - 1) * step_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_windows(x_data, y_data, window_size, step_size, augmentation_factor):\n",
    "    x_data_augmented = []\n",
    "    y_data_augmented = np.repeat(y_data, max(1, augmentation_factor))\n",
    "    if augmentation_factor <= 1:\n",
    "        return x_data, y_data_augmented\n",
    "    for i in range(x_data.shape[0]):\n",
    "        for j in range(0, x_data.shape[1] - window_size + 1, step_size):\n",
    "            single_window = x_data[i, j:j + window_size, :, :]\n",
    "            x_data_augmented.append(single_window)\n",
    "        \n",
    "    return np.array(x_data_augmented), y_data_augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_trail_count(x_data_list):\n",
    "    max_trail_count = 0\n",
    "    for x_data in x_data_list:\n",
    "        cur_trail_count = x_data.shape[0]\n",
    "        if cur_trail_count > max_trail_count:\n",
    "            max_trail_count = cur_trail_count\n",
    "    return max_trail_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def shuffle_lists(*lists):\n",
    "    for lst in lists:\n",
    "        random.shuffle(lst)\n",
    "\n",
    "def interleave_lists(list1, list2, list3):\n",
    "    len1, len2, len3 = len(list1), len(list2), len(list3)\n",
    "    \n",
    "    total_length = len1 + len2 + len3\n",
    "    \n",
    "    weight1 = len1 / total_length\n",
    "    weight2 = len2 / total_length\n",
    "    weight3 = len3 / total_length\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    used1 = used2 = used3 = 0\n",
    "    \n",
    "    while used1 + used2 + used3 < total_length:\n",
    "        ideal1 = (used1 + used2 + used3 + 1) * weight1\n",
    "        ideal2 = (used1 + used2 + used3 + 1) * weight2\n",
    "        ideal3 = (used1 + used2 + used3 + 1) * weight3\n",
    "        \n",
    "        diff1 = ideal1 - used1 if used1 < len1 else float('-inf')\n",
    "        diff2 = ideal2 - used2 if used2 < len2 else float('-inf')\n",
    "        diff3 = ideal3 - used3 if used3 < len3 else float('-inf')\n",
    "        \n",
    "        max_diff = max(diff1, diff2, diff3)\n",
    "        \n",
    "        if max_diff == diff1:\n",
    "            result.append(list1[used1])\n",
    "            used1 += 1\n",
    "        elif max_diff == diff2:\n",
    "            result.append(list2[used2])\n",
    "            used2 += 1\n",
    "        else:\n",
    "            result.append(list3[used3])\n",
    "            used3 += 1\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_1_x_data = get_preprocessed_x_data(ko_directory, ko_1_size, ko_1_file_prefix, True)\n",
    "wt_1_x_data = get_preprocessed_x_data(wt_directory, wt_1_size, wt_1_file_prefix, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_2_x_data = get_preprocessed_x_data(ko_directory, ko_2_size, ko_2_file_prefix, True)\n",
    "wt_2_x_data = get_preprocessed_x_data(wt_directory, wt_2_size, wt_2_file_prefix, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_3_x_data = get_preprocessed_x_data(ko_directory, ko_3_size, ko_3_file_prefix, True)\n",
    "wt_3_x_data = get_preprocessed_x_data(wt_directory, wt_3_size, wt_3_file_prefix, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_x_data = interleave_lists(ko_1_x_data, ko_2_x_data, ko_3_x_data)\n",
    "\n",
    "wt_x_data = interleave_lists(wt_1_x_data, wt_2_x_data, wt_3_x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_data_len = len(ko_x_data)\n",
    "wt_data_len = len(wt_x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_ko_x_data = padding_x_data(ko_x_data, max_trail_num)\n",
    "padded_wt_x_data = padding_x_data(wt_x_data, max_trail_num)\n",
    "all_x_data_list = padded_ko_x_data + padded_wt_x_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data, y_data = get_model_x_y(padded_ko_x_data, padded_wt_x_data, validation_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(146, 1280, 800, 6)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = get_sliding_window_size(x_data, data_augmentation_factor, step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1200"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = sliding_windows(x_data, y_data, window_size, step_size, data_augmentation_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "del ko_1_x_data\n",
    "del wt_1_x_data\n",
    "del ko_2_x_data\n",
    "del wt_2_x_data\n",
    "del ko_3_x_data\n",
    "del wt_3_x_data\n",
    "\n",
    "del ko_x_data\n",
    "del wt_x_data\n",
    "\n",
    "del padded_ko_x_data\n",
    "del padded_wt_x_data\n",
    "del all_x_data_list\n",
    "\n",
    "del x_data\n",
    "del y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(438, 1200, 800, 6)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, TimeDistributed, LSTM, GlobalAveragePooling1D, Flatten, Bidirectional, GRU, Conv1D, MaxPooling1D\n",
    "from keras.optimizers import AdamW\n",
    "# keras.optimizers.Adam runs slowly on M1,M2, so use keras.optimizers.legacy.Adam instead\n",
    "from keras.optimizers.legacy import Adam\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras.regularizers import l1, l2\n",
    "import tensorflow as tf\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "\n",
    "def get_sorted_error_sliced(y_true, y_pred):\n",
    "    \n",
    "    error = K.abs(y_pred - y_true)\n",
    "    error_transpose = tf.transpose(error)\n",
    "    sorted_error = tf.sort(error_transpose)\n",
    "    \n",
    "    num_samples = tf.shape(sorted_error)[1]\n",
    "    num_to_remove = num_samples // 5\n",
    "    \n",
    "    sorted_error_sliced = sorted_error[:, :-num_to_remove]\n",
    "    \n",
    "    return sorted_error_sliced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "\n",
    "def custom_error(y_true, y_pred):\n",
    "    \n",
    "    sorted_error_sliced = get_sorted_error_sliced(y_true, y_pred)\n",
    "    \n",
    "    mse = K.mean(K.square(sorted_error_sliced))\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "\n",
    "def custom_accuracy(y_true, y_pred):\n",
    "    sorted_error_sliced = get_sorted_error_sliced(y_true, y_pred)\n",
    "    \n",
    "    correct_predictions = 1 - K.abs(sorted_error_sliced)\n",
    "    \n",
    "    return K.mean(correct_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_mse(y_true, y_pred):\n",
    "    sorted_error_sliced = get_sorted_error_sliced(y_true, y_pred)\n",
    "\n",
    "    mse = K.mean(K.square(sorted_error_sliced))\n",
    "    \n",
    "    errors = K.abs(sorted_error_sliced)\n",
    "    \n",
    "    weights = K.exp(-K.square(errors))\n",
    "    \n",
    "    weighted_errors = K.square(errors) * weights\n",
    "    \n",
    "    weighted_mse = K.mean(weighted_errors)\n",
    "    \n",
    "    return weighted_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "\n",
    "def custom_binary_crossentropy(y_true, y_pred):\n",
    "    loss = binary_crossentropy(y_true, y_pred)\n",
    "    \n",
    "    sorted_indices = tf.argsort(loss, axis=0)\n",
    "    sorted_loss = tf.gather(loss, sorted_indices)\n",
    "    \n",
    "    num_data_points = tf.shape(loss)[0]\n",
    "    num_keep = num_data_points - num_data_points // 5\n",
    "    \n",
    "    reduced_loss = tf.reduce_mean(sorted_loss[:num_keep])\n",
    "    \n",
    "    return reduced_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def custom_accuracy(y_true, y_pred):\n",
    "    return K.mean(1 - K.abs(y_true - K.round(y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F1Score(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='f1_score', **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.precision = Precision()\n",
    "        self.recall = Recall()\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        self.precision.update_state(y_true, y_pred, sample_weight)\n",
    "        self.recall.update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "    def result(self):\n",
    "        p = self.precision.result()\n",
    "        r = self.recall.result()\n",
    "        return 2 * ((p * r) / (p + r + 1e-6))\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.precision.reset_states()\n",
    "        self.recall.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.metrics import Metric\n",
    "\n",
    "class MatthewsCorrelationCoefficient(Metric):\n",
    "    def __init__(self, name='mcc', threshold=0.5, **kwargs):\n",
    "        super(MatthewsCorrelationCoefficient, self).__init__(name=name, **kwargs)\n",
    "        self.threshold = threshold\n",
    "        self.true_positives = self.add_weight(name='tp', initializer='zeros')\n",
    "        self.true_negatives = self.add_weight(name='tn', initializer='zeros')\n",
    "        self.false_positives = self.add_weight(name='fp', initializer='zeros')\n",
    "        self.false_negatives = self.add_weight(name='fn', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred = tf.cast(y_pred > self.threshold, tf.float32)\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "\n",
    "        tp = tf.reduce_sum(y_true * y_pred)\n",
    "        tn = tf.reduce_sum((1 - y_true) * (1 - y_pred))\n",
    "        fp = tf.reduce_sum((1 - y_true) * y_pred)\n",
    "        fn = tf.reduce_sum(y_true * (1 - y_pred))\n",
    "\n",
    "        self.true_positives.assign_add(tp)\n",
    "        self.true_negatives.assign_add(tn)\n",
    "        self.false_positives.assign_add(fp)\n",
    "        self.false_negatives.assign_add(fn)\n",
    "\n",
    "    def result(self):\n",
    "        numerator = (self.true_positives * self.true_negatives - \n",
    "                    self.false_positives * self.false_negatives)\n",
    "        \n",
    "        denominator = tf.sqrt(\n",
    "            (self.true_positives + self.false_positives) *\n",
    "            (self.true_positives + self.false_negatives) *\n",
    "            (self.true_negatives + self.false_positives) *\n",
    "            (self.true_negatives + self.false_negatives) + \n",
    "            tf.keras.backend.epsilon()\n",
    "        )\n",
    "        \n",
    "        return numerator / denominator\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.true_positives.assign(0)\n",
    "        self.true_negatives.assign(0)\n",
    "        self.false_positives.assign(0)\n",
    "        self.false_negatives.assign(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.metrics import Metric, SpecificityAtSensitivity\n",
    "\n",
    "class BalancedAccuracy(Metric):\n",
    "    def __init__(self, name='balanced_acc', **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.recall = Recall()\n",
    "        self.specificity = SpecificityAtSensitivity(0.5)\n",
    "        \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        self.recall.update_state(y_true, y_pred, sample_weight)\n",
    "        self.specificity.update_state(y_true, y_pred, sample_weight)\n",
    "        \n",
    "    def result(self):\n",
    "        return (self.recall.result() + self.specificity.result()) / 2\n",
    "    \n",
    "    def reset_states(self):\n",
    "        self.recall.reset_states()\n",
    "        self.specificity.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Mixed precision compatibility check (mixed_float16): WARNING\n",
      "The dtype policy mixed_float16 may run slowly because this machine does not have a GPU. Only Nvidia GPUs with compute capability of at least 7.0 run quickly with mixed_float16.\n",
      "If you will use compatible GPU(s) not attached to this host, e.g. by running a multi-worker model, you can ignore this warning. This message will only be logged once\n"
     ]
    }
   ],
   "source": [
    "from keras.mixed_precision import set_global_policy\n",
    "set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed (TimeDist  (None, 1200, 800, 16)     2416      \n",
      " ributed)                                                        \n",
      "                                                                 \n",
      " time_distributed_1 (TimeDi  (None, 1200, 800, 16)     64        \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_2 (TimeDi  (None, 1200, 200, 16)     0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_3 (TimeDi  (None, 1200, 3200)        0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (None, 1200, 256)         3408896   \n",
      " al)                                                             \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1200, 256)         0         \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirecti  (None, 1200, 256)         394240    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 1200, 256)         0         \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 256)               0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                16448     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3822129 (14.58 MB)\n",
      "Trainable params: 3822097 (14.58 MB)\n",
      "Non-trainable params: 32 (128.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/13\n",
      "14/14 [==============================] - 164s 11s/step - loss: 5.8692 - accuracy: 0.7237 - precision: 0.6823 - recall: 0.8514 - auc: 0.7813 - f1_score: 0.7575 - mcc: 0.4603 - balanced_acc: 0.8285\n",
      "Epoch 2/13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yzc/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py:2723: UserWarning: Metric F1Score implements a `reset_states()` method; rename it to `reset_state()` (without the final \"s\"). The name `reset_states()` has been deprecated to improve API consistency.\n",
      "  m.reset_state()\n",
      "/Users/yzc/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py:2723: UserWarning: Metric MatthewsCorrelationCoefficient implements a `reset_states()` method; rename it to `reset_state()` (without the final \"s\"). The name `reset_states()` has been deprecated to improve API consistency.\n",
      "  m.reset_state()\n",
      "/Users/yzc/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py:2723: UserWarning: Metric BalancedAccuracy implements a `reset_states()` method; rename it to `reset_state()` (without the final \"s\"). The name `reset_states()` has been deprecated to improve API consistency.\n",
      "  m.reset_state()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 159s 11s/step - loss: 5.5572 - accuracy: 0.7374 - precision: 0.7336 - recall: 0.7568 - auc: 0.8089 - f1_score: 0.7450 - mcc: 0.4748 - balanced_acc: 0.7974\n",
      "Epoch 3/13\n",
      "14/14 [==============================] - 157s 11s/step - loss: 5.2975 - accuracy: 0.7329 - precision: 0.6923 - recall: 0.8514 - auc: 0.8033 - f1_score: 0.7636 - mcc: 0.4772 - balanced_acc: 0.8377\n",
      "Epoch 4/13\n",
      "14/14 [==============================] - 162s 12s/step - loss: 5.0419 - accuracy: 0.7397 - precision: 0.7061 - recall: 0.8333 - auc: 0.8104 - f1_score: 0.7645 - mcc: 0.4863 - balanced_acc: 0.8403\n",
      "Epoch 5/13\n",
      "14/14 [==============================] - 156s 11s/step - loss: 4.8034 - accuracy: 0.7671 - precision: 0.7174 - recall: 0.8919 - auc: 0.8180 - f1_score: 0.7952 - mcc: 0.5497 - balanced_acc: 0.8649\n",
      "Epoch 6/13\n",
      "14/14 [==============================] - 158s 11s/step - loss: 4.5839 - accuracy: 0.7603 - precision: 0.7259 - recall: 0.8468 - auc: 0.8053 - f1_score: 0.7817 - mcc: 0.5270 - balanced_acc: 0.8355\n",
      "Epoch 7/13\n",
      "14/14 [==============================] - 161s 11s/step - loss: 4.3626 - accuracy: 0.7626 - precision: 0.7235 - recall: 0.8604 - auc: 0.8263 - f1_score: 0.7860 - mcc: 0.5337 - balanced_acc: 0.8561\n",
      "Epoch 8/13\n",
      "14/14 [==============================] - 158s 11s/step - loss: 4.1645 - accuracy: 0.7580 - precision: 0.7086 - recall: 0.8874 - auc: 0.8274 - f1_score: 0.7880 - mcc: 0.5320 - balanced_acc: 0.8650\n",
      "Epoch 9/13\n",
      "14/14 [==============================] - 154s 11s/step - loss: 3.9850 - accuracy: 0.7671 - precision: 0.7190 - recall: 0.8874 - auc: 0.8213 - f1_score: 0.7944 - mcc: 0.5484 - balanced_acc: 0.8673\n",
      "Epoch 10/13\n",
      "14/14 [==============================] - 157s 11s/step - loss: 3.7902 - accuracy: 0.7740 - precision: 0.7338 - recall: 0.8694 - auc: 0.8421 - f1_score: 0.7959 - mcc: 0.5566 - balanced_acc: 0.8907\n",
      "Epoch 11/13\n",
      "14/14 [==============================] - 158s 11s/step - loss: 3.6386 - accuracy: 0.7877 - precision: 0.7549 - recall: 0.8604 - auc: 0.8355 - f1_score: 0.8042 - mcc: 0.5803 - balanced_acc: 0.8677\n",
      "Epoch 12/13\n",
      "14/14 [==============================] - 159s 11s/step - loss: 3.4655 - accuracy: 0.7922 - precision: 0.7417 - recall: 0.9054 - auc: 0.8520 - f1_score: 0.8154 - mcc: 0.5984 - balanced_acc: 0.8948\n",
      "Epoch 13/13\n",
      "14/14 [==============================] - 134s 9s/step - loss: 3.3229 - accuracy: 0.8037 - precision: 0.7556 - recall: 0.9054 - auc: 0.8532 - f1_score: 0.8238 - mcc: 0.6188 - balanced_acc: 0.8902\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "635"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from keras import backend as K\n",
    "from keras.metrics import Precision, Recall, AUC\n",
    "import time\n",
    "import pickle\n",
    "from keras.layers import LayerNormalization, MultiHeadAttention, GlobalMaxPooling1D, ReLU, BatchNormalization, Attention, Reshape, Conv1D, GlobalAveragePooling1D, Dense, Input, MultiHeadAttention\n",
    "\n",
    "K.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(TimeDistributed(Conv1D(16, kernel_size=25, activation='relu', \n",
    "       padding='same',\n",
    "       kernel_initializer='he_normal'),\n",
    "          input_shape=x_train.shape[1:]))\n",
    "model.add(TimeDistributed(BatchNormalization(momentum=0.95)))\n",
    "model.add(TimeDistributed(MaxPooling1D(4)))\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=True, kernel_regularizer=l2(0.002))))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=True, kernel_regularizer=l2(0.002))))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.005)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "optimizer = Adam(learning_rate = learning_rate)\n",
    "\n",
    "model.compile(optimizer = optimizer, loss = 'binary_crossentropy', metrics = [\n",
    "                  'accuracy', \n",
    "                  Precision(name='precision'), \n",
    "                  Recall(name='recall'),\n",
    "                  AUC(name='auc'),\n",
    "                  F1Score(),\n",
    "                  MatthewsCorrelationCoefficient(name='mcc'),\n",
    "                  BalancedAccuracy()])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'val_loss',\n",
    "                              factor = reduce_lr_factor,\n",
    "                              patience = reduce_lr_patience,\n",
    "                              min_lr = min_learning_rate)\n",
    "early_stopping = EarlyStopping(monitor = 'val_loss', patience = early_stopping_patience)\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size = batch_size, \n",
    "                    epochs = epochs)\n",
    "\n",
    "model.save('conv_bilstm_model.keras')\n",
    "with open('conv_bilstm_history.pkl', 'wb') as file_pi:\n",
    "    pickle.dump(history.history, file_pi)\n",
    "\n",
    "K.clear_session()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

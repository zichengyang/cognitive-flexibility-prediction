{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "CUiLU2CT-tb6"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "5Qqh9AfS-8ai"
   },
   "outputs": [],
   "source": [
    "ko_1_size = 15\n",
    "wt_1_size = 15\n",
    "ko_2_size = 41\n",
    "wt_2_size = 41\n",
    "ko_3_size = 16\n",
    "wt_3_size = 18\n",
    "\n",
    "ko_1_file_prefix = '1KO'\n",
    "wt_1_file_prefix = '1WT'\n",
    "ko_2_file_prefix = '2KO'\n",
    "wt_2_file_prefix = '2WT'\n",
    "ko_3_file_prefix = '3KO'\n",
    "wt_3_file_prefix = '3WT'\n",
    "\n",
    "ko_mice_symbol = 0\n",
    "wt_mice_symbol = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_trail_num = 1280\n",
    "# model params\n",
    "learning_rate = 0.00005\n",
    "reduce_lr_factor = 0.5\n",
    "reduce_lr_patience = 5\n",
    "min_learning_rate = 0.00001\n",
    "early_stopping_patience = 10\n",
    "batch_size = 32\n",
    "epochs = 20\n",
    "validation_split = 0.2\n",
    "metric_cut_percent = 0.2\n",
    "metric_lower_cut_percent = 0.2\n",
    "metric_upper_cut_percent = 0.2\n",
    "noise_deviation = 0.00005\n",
    "\n",
    "# data augmentation params\n",
    "data_augmentation_factor = 3\n",
    "step_size = 40\n",
    "test_data_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-8E4RgKo-8d1",
    "outputId": "49357805-0bbf-4bd7-eb1b-2ae407a7d135"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KO data dir exists: True\n",
      "WT data dir exists: True\n"
     ]
    }
   ],
   "source": [
    "ko_directory = r'/data/KO/'\n",
    "wt_directory = r'/data/WT/'\n",
    "print('KO data dir exists: ' + str(os.path.exists(ko_directory)))\n",
    "print('WT data dir exists: ' + str(os.path.exists(wt_directory)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0wkZDdks-8gD"
   },
   "outputs": [],
   "source": [
    "def read_x_data(file_dir, data_size, file_prefix):\n",
    "    train_x = []\n",
    "    train_record = []\n",
    "    for i in range(data_size):\n",
    "        train_x.append(pd.read_excel(os.path.join(file_dir, file_prefix + str(i+1) + '.xlsx'), dtype='int16', header=None, sheet_name='Sheet1'))\n",
    "        train_record.append(pd.read_excel(os.path.join(file_dir, file_prefix + str(i+1) + '.xlsx'), dtype='int16', header=None, sheet_name='Sheet2'))\n",
    "\n",
    "    return train_x, train_record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 修改时间步处理（示例：降采样到500步）\n",
    "# def downsample(data, time_factor=2):\n",
    "#     \"\"\"\n",
    "#     四维数据降采样：对时间步维度（第三维）进行降采样\n",
    "#     输入形状： (n_samples, trails, timesteps, features)\n",
    "#     输出形状： (n_samples, trails, timesteps//factor, features)\n",
    "#     \"\"\"\n",
    "#     return data[:, ::time_factor, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_data_pre_process(behavior_data_list, record_data_list, with_noise):\n",
    "    data_size = len(behavior_data_list)\n",
    "    pre_processed_data = []\n",
    "    \n",
    "    for i in range(data_size):\n",
    "        behavior_data = np.expand_dims(behavior_data_list[i].values[:, 200:1000], axis=-1)\n",
    "        record_data = record_data_list[i]\n",
    "        operation_data = record_data.loc[0]\n",
    "        odor1_data = np.zeros((behavior_data.shape[0], behavior_data.shape[1], 1))\n",
    "        odor2_data = np.zeros((behavior_data.shape[0], behavior_data.shape[1], 1))\n",
    "        reward_data = np.zeros((behavior_data.shape[0], behavior_data.shape[1], 1))\n",
    "        cur_trail_count_data = np.zeros((behavior_data.shape[0], behavior_data.shape[1], 1))\n",
    "        trail_count_data = np.zeros((behavior_data.shape[0], behavior_data.shape[1], 1))\n",
    "        \n",
    "        sum_trail_count = operation_data.shape[0]\n",
    "        for j in range(sum_trail_count):\n",
    "            lick_data = behavior_data[j]\n",
    "            trail_result = -1\n",
    "            odor = -1\n",
    "\n",
    "            if(operation_data[j]==1):\n",
    "                odor = 2\n",
    "                odor2_data[j, 0:100, :] = 1\n",
    "\n",
    "                # search lick index\n",
    "                lick_index = -1\n",
    "                for k in range(300,500):\n",
    "                    if(lick_data[k][0] == 1):\n",
    "                        lick_index = k\n",
    "                        break\n",
    "\n",
    "                if(lick_index > 0):\n",
    "                    reward_data[j, (lick_index +1):(lick_index + 51), :] = 1\n",
    "                    trail_result = 1\n",
    "                else:\n",
    "                    trail_result = 2\n",
    "            else:\n",
    "                odor = 1\n",
    "                odor1_data[j, 0:100, :] = 1\n",
    "\n",
    "                # search lick index\n",
    "                lick_index = -1\n",
    "                for k in range(0,600):\n",
    "                    if(lick_data[k]==1):\n",
    "                        lick_index = k\n",
    "                        break\n",
    "\n",
    "                # append result data\n",
    "                if(lick_index>0):\n",
    "                    trail_result = 3\n",
    "                else:\n",
    "                    trail_result = 4\n",
    "\n",
    "            cur_trail_count_data[j, :, :] = (j + 1) / sum_trail_count\n",
    "        \n",
    "        trail_count_data[:sum_trail_count, :, :] = sum_trail_count / 1850.0\n",
    "        x_data = np.concatenate((behavior_data, odor1_data, odor1_data, reward_data, cur_trail_count_data, trail_count_data), axis=2)\n",
    "        \n",
    "        if with_noise:\n",
    "            noise = np.random.normal(0, noise_deviation, x_data.shape)\n",
    "            x_data += noise\n",
    "\n",
    "        pre_processed_data.append(x_data)\n",
    "        \n",
    "    return pre_processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessed_x_data(file_dir, data_size, file_prefix, with_noise):\n",
    "    lick_data, record_data = read_x_data(file_dir, data_size, file_prefix)\n",
    "    pre_processed_data = train_data_pre_process(lick_data, record_data, with_noise)\n",
    "    return pre_processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_x_data(x_data_list, max_trail_count):\n",
    "    padded_x_data_list = []\n",
    "    for x_data in x_data_list:\n",
    "        if max_trail_count > len(x_data):\n",
    "            padding_size = max_trail_count - len(x_data)\n",
    "            padded = np.zeros((padding_size, x_data.shape[1], x_data.shape[2]))\n",
    "            padded_x_data = np.concatenate((x_data, padded), axis=0)\n",
    "            padded_x_data_list.append(padded_x_data)\n",
    "        else:\n",
    "            padded_x_data_list.append(x_data[:max_trail_count, :, :]) \n",
    "            \n",
    "    return padded_x_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_x_y(ko_x_data, wt_x_data, split=0.2):\n",
    "    ko_data_length = len(ko_x_data)\n",
    "    wt_data_length = len(wt_x_data)\n",
    "    ko_y_data = np.zeros(ko_data_length)\n",
    "    ko_y_data[:] = ko_mice_symbol\n",
    "    wt_y_data = np.zeros(wt_data_length)\n",
    "    wt_y_data[:] = wt_mice_symbol\n",
    "    \n",
    "    ko_split = int(ko_data_length * (1 - split))\n",
    "    wt_split = int(wt_data_length * (1 - split))\n",
    "    \n",
    "    ko_x_data = np.array(ko_x_data)\n",
    "    wt_x_data = np.array(wt_x_data)\n",
    "    \n",
    "    train_x = np.concatenate([ko_x_data[:ko_split], wt_x_data[:wt_split]])\n",
    "    train_y = np.concatenate([ko_y_data[:ko_split], wt_y_data[:wt_split]])\n",
    "    \n",
    "    val_x = np.concatenate([ko_x_data[ko_split:], wt_x_data[wt_split:]])\n",
    "    val_y = np.concatenate([ko_y_data[ko_split:], wt_y_data[wt_split:]])\n",
    "    \n",
    "    x_data = np.concatenate([train_x, val_x])\n",
    "    y_data = np.concatenate([train_y, val_y])\n",
    "    \n",
    "    return x_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_max_trail_count(data_array):\n",
    "    return max([data.shape[0] for data in data_array]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sliding_window_size(x_data, augmentation_factor, step_size):\n",
    "    trail_nums = [x.shape[0] for x in x_data]\n",
    "    max_trail_num = max(trail_nums)\n",
    "    if augmentation_factor <= 1:\n",
    "        return max_trail_num\n",
    "    return max_trail_num - (augmentation_factor - 1) * step_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_windows(x_data, y_data, window_size, step_size, augmentation_factor):\n",
    "    x_data_augmented = []\n",
    "    y_data_augmented = np.repeat(y_data, max(1, augmentation_factor))\n",
    "    if augmentation_factor <= 1:\n",
    "        return x_data, y_data_augmented\n",
    "    for i in range(x_data.shape[0]):\n",
    "        for j in range(0, x_data.shape[1] - window_size + 1, step_size):\n",
    "            single_window = x_data[i, j:j + window_size, :, :]\n",
    "            x_data_augmented.append(single_window)\n",
    "        \n",
    "    return np.array(x_data_augmented), y_data_augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_trail_count(x_data_list):\n",
    "    max_trail_count = 0\n",
    "    for x_data in x_data_list:\n",
    "        cur_trail_count = x_data.shape[0]\n",
    "        if cur_trail_count > max_trail_count:\n",
    "            max_trail_count = cur_trail_count\n",
    "    return max_trail_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def shuffle_lists(*lists):\n",
    "    for lst in lists:\n",
    "        random.shuffle(lst)\n",
    "\n",
    "def interleave_lists(list1, list2, list3):\n",
    "    len1, len2, len3 = len(list1), len(list2), len(list3)\n",
    "    \n",
    "    total_length = len1 + len2 + len3\n",
    "    \n",
    "    weight1 = len1 / total_length\n",
    "    weight2 = len2 / total_length\n",
    "    weight3 = len3 / total_length\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    used1 = used2 = used3 = 0\n",
    "    \n",
    "    while used1 + used2 + used3 < total_length:\n",
    "        ideal1 = (used1 + used2 + used3 + 1) * weight1\n",
    "        ideal2 = (used1 + used2 + used3 + 1) * weight2\n",
    "        ideal3 = (used1 + used2 + used3 + 1) * weight3\n",
    "        \n",
    "        diff1 = ideal1 - used1 if used1 < len1 else float('-inf')\n",
    "        diff2 = ideal2 - used2 if used2 < len2 else float('-inf')\n",
    "        diff3 = ideal3 - used3 if used3 < len3 else float('-inf')\n",
    "        \n",
    "        max_diff = max(diff1, diff2, diff3)\n",
    "        \n",
    "        if max_diff == diff1:\n",
    "            result.append(list1[used1])\n",
    "            used1 += 1\n",
    "        elif max_diff == diff2:\n",
    "            result.append(list2[used2])\n",
    "            used2 += 1\n",
    "        else:\n",
    "            result.append(list3[used3])\n",
    "            used3 += 1\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_1_x_data = get_preprocessed_x_data(ko_directory, ko_1_size, ko_1_file_prefix, True)\n",
    "wt_1_x_data = get_preprocessed_x_data(wt_directory, wt_1_size, wt_1_file_prefix, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_2_x_data = get_preprocessed_x_data(ko_directory, ko_2_size, ko_2_file_prefix, True)\n",
    "wt_2_x_data = get_preprocessed_x_data(wt_directory, wt_2_size, wt_2_file_prefix, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_3_x_data = get_preprocessed_x_data(ko_directory, ko_3_size, ko_3_file_prefix, True)\n",
    "wt_3_x_data = get_preprocessed_x_data(wt_directory, wt_3_size, wt_3_file_prefix, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_x_data = interleave_lists(ko_1_x_data, ko_2_x_data, ko_3_x_data)\n",
    "\n",
    "wt_x_data = interleave_lists(wt_1_x_data, wt_2_x_data, wt_3_x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_data_len = len(ko_x_data)\n",
    "wt_data_len = len(wt_x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_ko_x_data = padding_x_data(ko_x_data, max_trail_num)\n",
    "padded_wt_x_data = padding_x_data(wt_x_data, max_trail_num)\n",
    "all_x_data_list = padded_ko_x_data + padded_wt_x_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data, y_data = get_model_x_y(padded_ko_x_data, padded_wt_x_data, validation_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(146, 1280, 800, 6)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = get_sliding_window_size(x_data, data_augmentation_factor, step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1200"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = sliding_windows(x_data, y_data, window_size, step_size, data_augmentation_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "del ko_1_x_data\n",
    "del wt_1_x_data\n",
    "del ko_2_x_data\n",
    "del wt_2_x_data\n",
    "del ko_3_x_data\n",
    "del wt_3_x_data\n",
    "\n",
    "del ko_x_data\n",
    "del wt_x_data\n",
    "\n",
    "del padded_ko_x_data\n",
    "del padded_wt_x_data\n",
    "del all_x_data_list\n",
    "\n",
    "del x_data\n",
    "del y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(438, 1200, 800, 6)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, TimeDistributed, LSTM, GlobalAveragePooling1D, Flatten, Bidirectional, GRU, Conv1D, MaxPooling1D\n",
    "# from keras.optimizers import Adam\n",
    "# keras.optimizers.Adam runs slowly on M1,M2, so use keras.optimizers.legacy.Adam instead\n",
    "from keras.optimizers.legacy import Adam\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras.regularizers import l1, l2\n",
    "import tensorflow as tf\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "\n",
    "def get_sorted_error_sliced(y_true, y_pred):\n",
    "    \n",
    "    error = K.abs(y_pred - y_true)\n",
    "    error_transpose = tf.transpose(error)\n",
    "    sorted_error = tf.sort(error_transpose)\n",
    "    \n",
    "    num_samples = tf.shape(sorted_error)[1]\n",
    "    num_to_remove = num_samples // 5\n",
    "    \n",
    "    sorted_error_sliced = sorted_error[:, :-num_to_remove]\n",
    "    \n",
    "    return sorted_error_sliced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "\n",
    "def custom_error(y_true, y_pred):\n",
    "    \n",
    "    sorted_error_sliced = get_sorted_error_sliced(y_true, y_pred)\n",
    "    \n",
    "    mse = K.mean(K.square(sorted_error_sliced))\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "\n",
    "def custom_accuracy(y_true, y_pred):\n",
    "    sorted_error_sliced = get_sorted_error_sliced(y_true, y_pred)\n",
    "    \n",
    "    correct_predictions = 1 - K.abs(sorted_error_sliced)\n",
    "    \n",
    "    return K.mean(correct_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_mse(y_true, y_pred):\n",
    "    sorted_error_sliced = get_sorted_error_sliced(y_true, y_pred)\n",
    "\n",
    "    mse = K.mean(K.square(sorted_error_sliced))\n",
    "    \n",
    "    errors = K.abs(sorted_error_sliced)\n",
    "    \n",
    "    weights = K.exp(-K.square(errors))\n",
    "    \n",
    "    weighted_errors = K.square(errors) * weights\n",
    "    \n",
    "    weighted_mse = K.mean(weighted_errors)\n",
    "    \n",
    "    return weighted_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "\n",
    "def custom_binary_crossentropy(y_true, y_pred):\n",
    "    loss = binary_crossentropy(y_true, y_pred)\n",
    "    \n",
    "    sorted_indices = tf.argsort(loss, axis=0)\n",
    "    sorted_loss = tf.gather(loss, sorted_indices)\n",
    "    \n",
    "    num_data_points = tf.shape(loss)[0]\n",
    "    num_keep = num_data_points - num_data_points // 5\n",
    "    \n",
    "    reduced_loss = tf.reduce_mean(sorted_loss[:num_keep])\n",
    "    \n",
    "    return reduced_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def custom_accuracy(y_true, y_pred):\n",
    "    return K.mean(1 - K.abs(y_true - K.round(y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F1Score(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='f1_score', **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.precision = Precision()\n",
    "        self.recall = Recall()\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        self.precision.update_state(y_true, y_pred, sample_weight)\n",
    "        self.recall.update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "    def result(self):\n",
    "        p = self.precision.result()\n",
    "        r = self.recall.result()\n",
    "        return 2 * ((p * r) / (p + r + 1e-6))\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.precision.reset_states()\n",
    "        self.recall.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.metrics import Metric\n",
    "\n",
    "class MatthewsCorrelationCoefficient(Metric):\n",
    "    def __init__(self, name='mcc', threshold=0.5, **kwargs):\n",
    "        super(MatthewsCorrelationCoefficient, self).__init__(name=name, **kwargs)\n",
    "        self.threshold = threshold\n",
    "        self.true_positives = self.add_weight(name='tp', initializer='zeros')\n",
    "        self.true_negatives = self.add_weight(name='tn', initializer='zeros')\n",
    "        self.false_positives = self.add_weight(name='fp', initializer='zeros')\n",
    "        self.false_negatives = self.add_weight(name='fn', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred = tf.cast(y_pred > self.threshold, tf.float32)\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "\n",
    "        tp = tf.reduce_sum(y_true * y_pred)\n",
    "        tn = tf.reduce_sum((1 - y_true) * (1 - y_pred))\n",
    "        fp = tf.reduce_sum((1 - y_true) * y_pred)\n",
    "        fn = tf.reduce_sum(y_true * (1 - y_pred))\n",
    "\n",
    "        self.true_positives.assign_add(tp)\n",
    "        self.true_negatives.assign_add(tn)\n",
    "        self.false_positives.assign_add(fp)\n",
    "        self.false_negatives.assign_add(fn)\n",
    "\n",
    "    def result(self):\n",
    "        numerator = (self.true_positives * self.true_negatives - \n",
    "                    self.false_positives * self.false_negatives)\n",
    "        \n",
    "        denominator = tf.sqrt(\n",
    "            (self.true_positives + self.false_positives) *\n",
    "            (self.true_positives + self.false_negatives) *\n",
    "            (self.true_negatives + self.false_positives) *\n",
    "            (self.true_negatives + self.false_negatives) + \n",
    "            tf.keras.backend.epsilon()\n",
    "        )\n",
    "        \n",
    "        return numerator / denominator\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.true_positives.assign(0)\n",
    "        self.true_negatives.assign(0)\n",
    "        self.false_positives.assign(0)\n",
    "        self.false_negatives.assign(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.metrics import Metric, SpecificityAtSensitivity\n",
    "\n",
    "class BalancedAccuracy(Metric):\n",
    "    def __init__(self, name='balanced_acc', **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.recall = Recall()\n",
    "        self.specificity = SpecificityAtSensitivity(0.5)\n",
    "        \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        self.recall.update_state(y_true, y_pred, sample_weight)\n",
    "        self.specificity.update_state(y_true, y_pred, sample_weight)\n",
    "        \n",
    "    def result(self):\n",
    "        return (self.recall.result() + self.specificity.result()) / 2\n",
    "    \n",
    "    def reset_states(self):\n",
    "        self.recall.reset_states()\n",
    "        self.specificity.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation class distribution: [43 45]\n"
     ]
    }
   ],
   "source": [
    "y_val = y_train[int((1-validation_split) *len(y_train)):]\n",
    "print(\"Validation class distribution:\", np.bincount(y_val.astype(int)))\n",
    "\n",
    "if len(np.unique(y_val)) == 1:\n",
    "    print(\"警告：验证集只有单一类别！需调整数据分割策略\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Mixed precision compatibility check (mixed_float16): WARNING\n",
      "The dtype policy mixed_float16 may run slowly because this machine does not have a GPU. Only Nvidia GPUs with compute capability of at least 7.0 run quickly with mixed_float16.\n",
      "If you will use compatible GPU(s) not attached to this host, e.g. by running a multi-worker model, you can ignore this warning. This message will only be logged once\n"
     ]
    }
   ],
   "source": [
    "from keras.mixed_precision import set_global_policy\n",
    "set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from keras import backend as K\n",
    "from keras.metrics import Precision, Recall, AUC\n",
    "import time\n",
    "import pickle\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import SpatialDropout1D, Add, Activation, GlobalMaxPooling1D, ReLU, BatchNormalization, GRU, Attention, Reshape, Conv1D, GlobalAveragePooling1D, Dense, Input, MultiHeadAttention, LayerNormalization, GaussianNoise\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "# from tensorflow_addons.layers import WeightNormalization\n",
    "\n",
    "current_seed = 1028\n",
    "print(current_seed)\n",
    "k = 5\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=1028)\n",
    "histories = []\n",
    "\n",
    "cross_val_results = {\n",
    "    'config': {\n",
    "        'seed': current_seed,\n",
    "        'batch_size': batch_size,\n",
    "        'learning_rate': learning_rate,\n",
    "    },\n",
    "    'folds': []\n",
    "}\n",
    "\n",
    "\n",
    "def build_tcn():\n",
    "    inputs = Input(shape=(1200, 800, 6))\n",
    "    \n",
    "    x = Reshape((1200, 800*6))(inputs)\n",
    "\n",
    "    def residual_block(x_in, dilation_rate, filters=64, kernel_size=3):\n",
    "        x = Conv1D(filters, kernel_size, \n",
    "                  padding='causal',\n",
    "                  dilation_rate=dilation_rate,\n",
    "                  kernel_initializer='he_normal')(x_in)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('swish')(x)  \n",
    "        x = SpatialDropout1D(0.3)(x)\n",
    "\n",
    "        x = Conv1D(filters, kernel_size, \n",
    "                  padding='causal',\n",
    "                  dilation_rate=dilation_rate,\n",
    "                  kernel_initializer='he_normal')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "\n",
    "        if x_in.shape[-1] != filters:\n",
    "            res = Conv1D(filters, 1, kernel_initializer='he_normal')(x_in)\n",
    "        else:\n",
    "            res = x_in\n",
    "\n",
    "        out = Add()([x, res])\n",
    "        return Activation('swish')(out)\n",
    "    \n",
    "    x = residual_block(x, dilation_rate=1)\n",
    "    x = Activation('swish')(x)\n",
    "    x = residual_block(x, dilation_rate=2)\n",
    "    x = Activation('swish')(x)\n",
    "    x = residual_block(x, dilation_rate=4)\n",
    "    x = Activation('swish')(x)\n",
    "    x = residual_block(x, dilation_rate=8)\n",
    "    x = Activation('swish')(x)\n",
    "    \n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dense(32, activation='relu', kernel_regularizer=l2(1e-3))(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    return Model(inputs, outputs)\n",
    "    \n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(x_train)):\n",
    "    print(f\"\\nTraining Fold {fold+1}/{k}\")\n",
    "    \n",
    "    K.clear_session()\n",
    "    gc.collect()\n",
    "    \n",
    "    model = build_tcn()\n",
    "\n",
    "    optimizer = Adam(learning_rate = learning_rate)\n",
    "    model.compile(optimizer = optimizer, loss = 'binary_crossentropy', metrics = [\n",
    "                      'accuracy', \n",
    "                      Precision(name='precision'), \n",
    "                      Recall(name='recall'),\n",
    "                      AUC(name='auc'),\n",
    "                      F1Score(),\n",
    "                      MatthewsCorrelationCoefficient(name='mcc'),\n",
    "                      BalancedAccuracy()])\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    reduce_lr = ReduceLROnPlateau(monitor = 'val_loss',\n",
    "                                  factor = reduce_lr_factor,\n",
    "                                  patience = reduce_lr_patience,\n",
    "                                  min_lr = min_learning_rate)\n",
    "    early_stopping = EarlyStopping(monitor = 'val_loss', patience = early_stopping_patience)\n",
    "    \n",
    "    x_val_fold = x_train[val_idx]\n",
    "    y_val_fold = y_train[val_idx]\n",
    "\n",
    "    history = model.fit(x_train[train_idx], y_train[train_idx],\n",
    "                        validation_data=(x_val_fold, y_val_fold), \n",
    "                        batch_size = batch_size, \n",
    "                        epochs = epochs,\n",
    "                        callbacks=[early_stopping, reduce_lr])\n",
    "    \n",
    "    best_epoch = np.argmin(history.history['val_loss']) + 1\n",
    "    \n",
    "    y_pred_probs = model.predict(x_val_fold).flatten()\n",
    "    \n",
    "    fold_data = {\n",
    "        'fold_number': fold+1,\n",
    "        'train_indices': train_idx,\n",
    "        'val_indices': val_idx,\n",
    "        'history': history.history,\n",
    "        'best_epoch': best_epoch,\n",
    "        'y_true': y_val_fold,\n",
    "        'y_pred_probs': y_pred_probs,\n",
    "        'final_metrics': dict(zip(\n",
    "            model.metrics_names,\n",
    "            model.evaluate(x_val_fold, y_val_fold, verbose=0)\n",
    "        ))\n",
    "    }\n",
    "    \n",
    "    model.save(f'tcn_fold_{fold+1}.keras')\n",
    "    with open(f'tcn_fold_{fold+1}_data.pkl', 'wb') as f:\n",
    "        pickle.dump(fold_data, f)\n",
    "    \n",
    "    # 添加到总结果\n",
    "    cross_val_results['folds'].append(fold_data)\n",
    "\n",
    "    print(\"evaluate result\")\n",
    "    model.evaluate(x_train[val_idx], y_train[val_idx], verbose=0)\n",
    "    # 保存 history\n",
    "    with open(f'tcn_fold_{fold+1}_history.pkl', 'wb') as file_pi:\n",
    "        pickle.dump(history.history, file_pi)\n",
    "        \n",
    "    model.save(r'tcn_fold_'+str(fold+1) +'.keras')\n",
    "        \n",
    "    del model\n",
    "    K.clear_session()\n",
    "    gc.collect()\n",
    "    \n",
    "with open('tcn_cross_validation_results.pkl', 'wb') as f:\n",
    "    pickle.dump(cross_val_results, f)\n",
    "print(\"\\n所有交叉验证轮次完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "CUiLU2CT-tb6"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "5Qqh9AfS-8ai"
   },
   "outputs": [],
   "source": [
    "ko_1_size = 15\n",
    "wt_1_size = 15\n",
    "ko_2_size = 41\n",
    "wt_2_size = 41\n",
    "ko_3_size = 16\n",
    "wt_3_size = 18\n",
    "\n",
    "ko_1_file_prefix = '1KO'\n",
    "wt_1_file_prefix = '1WT'\n",
    "ko_2_file_prefix = '2KO'\n",
    "wt_2_file_prefix = '2WT'\n",
    "ko_3_file_prefix = '3KO'\n",
    "wt_3_file_prefix = '3WT'\n",
    "\n",
    "ko_mice_symbol = 0\n",
    "wt_mice_symbol = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_trail_num = 1280\n",
    "# model params\n",
    "learning_rate = 0.0001\n",
    "reduce_lr_factor = 0.5\n",
    "reduce_lr_patience = 5\n",
    "min_learning_rate = 0.00001\n",
    "early_stopping_patience = 10\n",
    "batch_size = 32\n",
    "epochs = 20\n",
    "validation_split = 0.2\n",
    "metric_cut_percent = 0.2\n",
    "metric_lower_cut_percent = 0.2\n",
    "metric_upper_cut_percent = 0.2\n",
    "noise_deviation = 0.00005\n",
    "\n",
    "# data augmentation params\n",
    "data_augmentation_factor = 3\n",
    "step_size = 40\n",
    "test_data_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-8E4RgKo-8d1",
    "outputId": "49357805-0bbf-4bd7-eb1b-2ae407a7d135"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KO data dir exists: True\n",
      "WT data dir exists: True\n"
     ]
    }
   ],
   "source": [
    "ko_directory = r'/data/KO/'\n",
    "wt_directory = r'/data/WT/'\n",
    "print('KO data dir exists: ' + str(os.path.exists(ko_directory)))\n",
    "print('WT data dir exists: ' + str(os.path.exists(wt_directory)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0wkZDdks-8gD"
   },
   "outputs": [],
   "source": [
    "def read_x_data(file_dir, data_size, file_prefix):\n",
    "    train_x = []\n",
    "    train_record = []\n",
    "    for i in range(data_size):\n",
    "        train_x.append(pd.read_excel(os.path.join(file_dir, file_prefix + str(i+1) + '.xlsx'), dtype='int16', header=None, sheet_name='Sheet1'))\n",
    "        train_record.append(pd.read_excel(os.path.join(file_dir, file_prefix + str(i+1) + '.xlsx'), dtype='int16', header=None, sheet_name='Sheet2'))\n",
    "\n",
    "    return train_x, train_record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def downsample(data, time_factor=2):\n",
    "#     \"\"\"\n",
    "#     input shape： (n_samples, trails, timesteps, features)\n",
    "#     output shape： (n_samples, trails, timesteps//factor, features)\n",
    "#     \"\"\"\n",
    "#     return data[:, ::time_factor, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_data_pre_process(behavior_data_list, record_data_list, with_noise):\n",
    "    data_size = len(behavior_data_list)\n",
    "    pre_processed_data = []\n",
    "    \n",
    "    for i in range(data_size):\n",
    "        behavior_data = np.expand_dims(behavior_data_list[i].values[:, 200:1000], axis=-1)\n",
    "        record_data = record_data_list[i]\n",
    "        operation_data = record_data.loc[0]\n",
    "        odor1_data = np.zeros((behavior_data.shape[0], behavior_data.shape[1], 1))\n",
    "        odor2_data = np.zeros((behavior_data.shape[0], behavior_data.shape[1], 1))\n",
    "        reward_data = np.zeros((behavior_data.shape[0], behavior_data.shape[1], 1))\n",
    "        cur_trail_count_data = np.zeros((behavior_data.shape[0], behavior_data.shape[1], 1))\n",
    "        trail_count_data = np.zeros((behavior_data.shape[0], behavior_data.shape[1], 1))\n",
    "        \n",
    "        sum_trail_count = operation_data.shape[0]\n",
    "        for j in range(sum_trail_count):\n",
    "            lick_data = behavior_data[j]\n",
    "            trail_result = -1\n",
    "            odor = -1\n",
    "\n",
    "            if(operation_data[j]==1):\n",
    "                odor = 2\n",
    "                odor2_data[j, 0:100, :] = 1\n",
    "\n",
    "                # search lick index\n",
    "                lick_index = -1\n",
    "                for k in range(300,500):\n",
    "                    if(lick_data[k][0] == 1):\n",
    "                        lick_index = k\n",
    "                        break\n",
    "\n",
    "                if(lick_index > 0):\n",
    "                    reward_data[j, (lick_index +1):(lick_index + 51), :] = 1\n",
    "                    trail_result = 1\n",
    "                else:\n",
    "                    trail_result = 2\n",
    "            else:\n",
    "                odor = 1\n",
    "                odor1_data[j, 0:100, :] = 1\n",
    "\n",
    "                # search lick index\n",
    "                lick_index = -1\n",
    "                for k in range(0,600):\n",
    "                    if(lick_data[k]==1):\n",
    "                        lick_index = k\n",
    "                        break\n",
    "\n",
    "                # append result data\n",
    "                if(lick_index>0):\n",
    "                    trail_result = 3\n",
    "                else:\n",
    "                    trail_result = 4\n",
    "\n",
    "            cur_trail_count_data[j, :, :] = (j + 1) / sum_trail_count\n",
    "        \n",
    "        trail_count_data[:sum_trail_count, :, :] = sum_trail_count / 1850.0\n",
    "        x_data = np.concatenate((behavior_data, odor1_data, odor1_data, reward_data, cur_trail_count_data, trail_count_data), axis=2)\n",
    "        \n",
    "        if with_noise:\n",
    "            noise = np.random.normal(0, noise_deviation, x_data.shape)\n",
    "            x_data += noise\n",
    "\n",
    "        pre_processed_data.append(x_data)\n",
    "        \n",
    "    return pre_processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessed_x_data(file_dir, data_size, file_prefix, with_noise):\n",
    "    lick_data, record_data = read_x_data(file_dir, data_size, file_prefix)\n",
    "    pre_processed_data = train_data_pre_process(lick_data, record_data, with_noise)\n",
    "    return pre_processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_x_data(x_data_list, max_trail_count):\n",
    "    padded_x_data_list = []\n",
    "    for x_data in x_data_list:\n",
    "        if max_trail_count > len(x_data):\n",
    "            padding_size = max_trail_count - len(x_data)\n",
    "            padded = np.zeros((padding_size, x_data.shape[1], x_data.shape[2]))  # 使用 0 进行填充\n",
    "            padded_x_data = np.concatenate((x_data, padded), axis=0)\n",
    "            padded_x_data_list.append(padded_x_data)\n",
    "        else:\n",
    "            padded_x_data_list.append(x_data[:max_trail_count, :, :]) \n",
    "            \n",
    "    return padded_x_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_x_y(ko_x_data, wt_x_data, split=0.2):\n",
    "    ko_data_length = len(ko_x_data)\n",
    "    wt_data_length = len(wt_x_data)\n",
    "    ko_y_data = np.zeros(ko_data_length)\n",
    "    ko_y_data[:] = ko_mice_symbol\n",
    "    wt_y_data = np.zeros(wt_data_length)\n",
    "    wt_y_data[:] = wt_mice_symbol\n",
    "    \n",
    "    ko_split = int(ko_data_length * (1 - split))\n",
    "    wt_split = int(wt_data_length * (1 - split))\n",
    "    \n",
    "    ko_x_data = np.array(ko_x_data)\n",
    "    wt_x_data = np.array(wt_x_data)\n",
    "    \n",
    "    train_x = np.concatenate([ko_x_data[:ko_split], wt_x_data[:wt_split]])\n",
    "    train_y = np.concatenate([ko_y_data[:ko_split], wt_y_data[:wt_split]])\n",
    "    \n",
    "    val_x = np.concatenate([ko_x_data[ko_split:], wt_x_data[wt_split:]])\n",
    "    val_y = np.concatenate([ko_y_data[ko_split:], wt_y_data[wt_split:]])\n",
    "    \n",
    "    x_data = np.concatenate([train_x, val_x])\n",
    "    y_data = np.concatenate([train_y, val_y])\n",
    "    \n",
    "    return x_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_max_trail_count(data_array):\n",
    "    return max([data.shape[0] for data in data_array]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sliding_window_size(x_data, augmentation_factor, step_size):\n",
    "    trail_nums = [x.shape[0] for x in x_data]\n",
    "    max_trail_num = max(trail_nums)\n",
    "    if augmentation_factor <= 1:\n",
    "        return max_trail_num\n",
    "    return max_trail_num - (augmentation_factor - 1) * step_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_windows(x_data, y_data, window_size, step_size, augmentation_factor):\n",
    "    x_data_augmented = []\n",
    "    y_data_augmented = np.repeat(y_data, max(1, augmentation_factor))\n",
    "    if augmentation_factor <= 1:\n",
    "        return x_data, y_data_augmented\n",
    "    for i in range(x_data.shape[0]):\n",
    "        for j in range(0, x_data.shape[1] - window_size + 1, step_size):\n",
    "            single_window = x_data[i, j:j + window_size, :, :]\n",
    "            x_data_augmented.append(single_window)\n",
    "        \n",
    "    return np.array(x_data_augmented), y_data_augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_trail_count(x_data_list):\n",
    "    max_trail_count = 0\n",
    "    for x_data in x_data_list:\n",
    "        cur_trail_count = x_data.shape[0]\n",
    "        if cur_trail_count > max_trail_count:\n",
    "            max_trail_count = cur_trail_count\n",
    "    return max_trail_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def shuffle_lists(*lists):\n",
    "    for lst in lists:\n",
    "        random.shuffle(lst)\n",
    "\n",
    "def interleave_lists(list1, list2, list3):\n",
    "    len1, len2, len3 = len(list1), len(list2), len(list3)\n",
    "    \n",
    "    total_length = len1 + len2 + len3\n",
    "    \n",
    "    weight1 = len1 / total_length\n",
    "    weight2 = len2 / total_length\n",
    "    weight3 = len3 / total_length\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    used1 = used2 = used3 = 0\n",
    "    \n",
    "    while used1 + used2 + used3 < total_length:\n",
    "        ideal1 = (used1 + used2 + used3 + 1) * weight1\n",
    "        ideal2 = (used1 + used2 + used3 + 1) * weight2\n",
    "        ideal3 = (used1 + used2 + used3 + 1) * weight3\n",
    "        \n",
    "        diff1 = ideal1 - used1 if used1 < len1 else float('-inf')\n",
    "        diff2 = ideal2 - used2 if used2 < len2 else float('-inf')\n",
    "        diff3 = ideal3 - used3 if used3 < len3 else float('-inf')\n",
    "        \n",
    "        max_diff = max(diff1, diff2, diff3)\n",
    "        \n",
    "        if max_diff == diff1:\n",
    "            result.append(list1[used1])\n",
    "            used1 += 1\n",
    "        elif max_diff == diff2:\n",
    "            result.append(list2[used2])\n",
    "            used2 += 1\n",
    "        else:\n",
    "            result.append(list3[used3])\n",
    "            used3 += 1\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_1_x_data = get_preprocessed_x_data(ko_directory, ko_1_size, ko_1_file_prefix, True)\n",
    "wt_1_x_data = get_preprocessed_x_data(wt_directory, wt_1_size, wt_1_file_prefix, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_2_x_data = get_preprocessed_x_data(ko_directory, ko_2_size, ko_2_file_prefix, True)\n",
    "wt_2_x_data = get_preprocessed_x_data(wt_directory, wt_2_size, wt_2_file_prefix, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_3_x_data = get_preprocessed_x_data(ko_directory, ko_3_size, ko_3_file_prefix, True)\n",
    "wt_3_x_data = get_preprocessed_x_data(wt_directory, wt_3_size, wt_3_file_prefix, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_x_data = interleave_lists(ko_1_x_data, ko_2_x_data, ko_3_x_data)\n",
    "\n",
    "wt_x_data = interleave_lists(wt_1_x_data, wt_2_x_data, wt_3_x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_data_len = len(ko_x_data)\n",
    "wt_data_len = len(wt_x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_ko_x_data = padding_x_data(ko_x_data, max_trail_num)\n",
    "padded_wt_x_data = padding_x_data(wt_x_data, max_trail_num)\n",
    "all_x_data_list = padded_ko_x_data + padded_wt_x_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data, y_data = get_model_x_y(padded_ko_x_data, padded_wt_x_data, validation_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(146, 1280, 800, 6)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = get_sliding_window_size(x_data, data_augmentation_factor, step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1200"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = sliding_windows(x_data, y_data, window_size, step_size, data_augmentation_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "del ko_1_x_data\n",
    "del wt_1_x_data\n",
    "del ko_2_x_data\n",
    "del wt_2_x_data\n",
    "del ko_3_x_data\n",
    "del wt_3_x_data\n",
    "\n",
    "del ko_x_data\n",
    "del wt_x_data\n",
    "\n",
    "del padded_ko_x_data\n",
    "del padded_wt_x_data\n",
    "del all_x_data_list\n",
    "\n",
    "del x_data\n",
    "del y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(438, 1200, 800, 6)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, TimeDistributed, LSTM, GlobalAveragePooling1D, Flatten, Bidirectional, GRU, Conv1D, MaxPooling1D\n",
    "# from keras.optimizers import Adam\n",
    "# keras.optimizers.Adam runs slowly on M1,M2, so use keras.optimizers.legacy.Adam instead\n",
    "from keras.optimizers.legacy import Adam\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras.regularizers import l1, l2\n",
    "import tensorflow as tf\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "\n",
    "def get_sorted_error_sliced(y_true, y_pred):\n",
    "    \n",
    "    error = K.abs(y_pred - y_true)\n",
    "    error_transpose = tf.transpose(error)\n",
    "    sorted_error = tf.sort(error_transpose)\n",
    "    \n",
    "    num_samples = tf.shape(sorted_error)[1]\n",
    "    num_to_remove = num_samples // 5\n",
    "    \n",
    "    sorted_error_sliced = sorted_error[:, :-num_to_remove]\n",
    "    \n",
    "    return sorted_error_sliced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "\n",
    "def custom_error(y_true, y_pred):\n",
    "    \n",
    "    sorted_error_sliced = get_sorted_error_sliced(y_true, y_pred)\n",
    "    \n",
    "    mse = K.mean(K.square(sorted_error_sliced))\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "\n",
    "def custom_accuracy(y_true, y_pred):\n",
    "    sorted_error_sliced = get_sorted_error_sliced(y_true, y_pred)\n",
    "    \n",
    "    correct_predictions = 1 - K.abs(sorted_error_sliced)\n",
    "    \n",
    "    return K.mean(correct_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_mse(y_true, y_pred):\n",
    "    sorted_error_sliced = get_sorted_error_sliced(y_true, y_pred)\n",
    "\n",
    "    mse = K.mean(K.square(sorted_error_sliced))\n",
    "    \n",
    "    errors = K.abs(sorted_error_sliced)\n",
    "    \n",
    "    weights = K.exp(-K.square(errors))\n",
    "    \n",
    "    weighted_errors = K.square(errors) * weights\n",
    "    \n",
    "    weighted_mse = K.mean(weighted_errors)\n",
    "    \n",
    "    return weighted_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "\n",
    "def custom_binary_crossentropy(y_true, y_pred):\n",
    "    loss = binary_crossentropy(y_true, y_pred)\n",
    "    \n",
    "    sorted_indices = tf.argsort(loss, axis=0)\n",
    "    sorted_loss = tf.gather(loss, sorted_indices)\n",
    "    \n",
    "    num_data_points = tf.shape(loss)[0]\n",
    "    num_keep = num_data_points - num_data_points // 5\n",
    "    \n",
    "    reduced_loss = tf.reduce_mean(sorted_loss[:num_keep])\n",
    "    \n",
    "    return reduced_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def custom_accuracy(y_true, y_pred):\n",
    "    return K.mean(1 - K.abs(y_true - K.round(y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F1Score(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='f1_score', **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.precision = Precision()\n",
    "        self.recall = Recall()\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        self.precision.update_state(y_true, y_pred, sample_weight)\n",
    "        self.recall.update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "    def result(self):\n",
    "        p = self.precision.result()\n",
    "        r = self.recall.result()\n",
    "        return 2 * ((p * r) / (p + r + 1e-6))\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.precision.reset_states()\n",
    "        self.recall.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.metrics import Metric\n",
    "\n",
    "class MatthewsCorrelationCoefficient(Metric):\n",
    "    def __init__(self, name='mcc', threshold=0.5, **kwargs):\n",
    "        super(MatthewsCorrelationCoefficient, self).__init__(name=name, **kwargs)\n",
    "        self.threshold = threshold\n",
    "        self.true_positives = self.add_weight(name='tp', initializer='zeros')\n",
    "        self.true_negatives = self.add_weight(name='tn', initializer='zeros')\n",
    "        self.false_positives = self.add_weight(name='fp', initializer='zeros')\n",
    "        self.false_negatives = self.add_weight(name='fn', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred = tf.cast(y_pred > self.threshold, tf.float32)\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "\n",
    "        tp = tf.reduce_sum(y_true * y_pred)\n",
    "        tn = tf.reduce_sum((1 - y_true) * (1 - y_pred))\n",
    "        fp = tf.reduce_sum((1 - y_true) * y_pred)\n",
    "        fn = tf.reduce_sum(y_true * (1 - y_pred))\n",
    "\n",
    "        self.true_positives.assign_add(tp)\n",
    "        self.true_negatives.assign_add(tn)\n",
    "        self.false_positives.assign_add(fp)\n",
    "        self.false_negatives.assign_add(fn)\n",
    "\n",
    "    def result(self):\n",
    "        numerator = (self.true_positives * self.true_negatives - \n",
    "                    self.false_positives * self.false_negatives)\n",
    "        \n",
    "        denominator = tf.sqrt(\n",
    "            (self.true_positives + self.false_positives) *\n",
    "            (self.true_positives + self.false_negatives) *\n",
    "            (self.true_negatives + self.false_positives) *\n",
    "            (self.true_negatives + self.false_negatives) + \n",
    "            tf.keras.backend.epsilon()\n",
    "        )\n",
    "        \n",
    "        return numerator / denominator\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.true_positives.assign(0)\n",
    "        self.true_negatives.assign(0)\n",
    "        self.false_positives.assign(0)\n",
    "        self.false_negatives.assign(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.metrics import Metric, SpecificityAtSensitivity\n",
    "\n",
    "class BalancedAccuracy(Metric):\n",
    "    def __init__(self, name='balanced_acc', **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.recall = Recall()\n",
    "        self.specificity = SpecificityAtSensitivity(0.5)\n",
    "        \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        self.recall.update_state(y_true, y_pred, sample_weight)\n",
    "        self.specificity.update_state(y_true, y_pred, sample_weight)\n",
    "        \n",
    "    def result(self):\n",
    "        return (self.recall.result() + self.specificity.result()) / 2\n",
    "    \n",
    "    def reset_states(self):\n",
    "        self.recall.reset_states()\n",
    "        self.specificity.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation class distribution: [43 45]\n"
     ]
    }
   ],
   "source": [
    "y_val = y_train[int((1-validation_split) *len(y_train)):]\n",
    "print(\"Validation class distribution:\", np.bincount(y_val.astype(int)))\n",
    "\n",
    "if len(np.unique(y_val)) == 1:\n",
    "    print(\"警告：验证集只有单一类别！需调整数据分割策略\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Mixed precision compatibility check (mixed_float16): WARNING\n",
      "The dtype policy mixed_float16 may run slowly because this machine does not have a GPU. Only Nvidia GPUs with compute capability of at least 7.0 run quickly with mixed_float16.\n",
      "If you will use compatible GPU(s) not attached to this host, e.g. by running a multi-worker model, you can ignore this warning. This message will only be logged once\n"
     ]
    }
   ],
   "source": [
    "from keras.mixed_precision import set_global_policy\n",
    "set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# def preprocess_features(x_data):\n",
    "#     \"\"\"\n",
    "#     input shape (samples, trails, timesteps, 6)\n",
    "#     output shape (samples, trails, timesteps, 6)\n",
    "#     \"\"\"\n",
    "#     processed = np.zeros_like(x_data, dtype=np.float32)\n",
    "    \n",
    "#     processed[..., :3] = x_data[..., :3]  # lick, odor1, odor2\n",
    "    \n",
    "#     processed[..., 3] = (x_data[..., 3] > 0).astype(np.float32)\n",
    "    \n",
    "#     for sample_idx in range(x_data.shape[0]):\n",
    "#         trail_scaler = MinMaxScaler()\n",
    "#         trails = x_data[sample_idx, :, 0, 4]\n",
    "#         processed[sample_idx, :, :, 4] = trail_scaler.fit_transform(trails.reshape(-1,1))\n",
    "    \n",
    "#     total_trail_scaler = MinMaxScaler()\n",
    "#     total_trails = x_data[:, 0, 0, 5].reshape(-1,1)\n",
    "#     global_norm = total_trail_scaler.fit_transform(total_trails)\n",
    "#     processed[..., 5] = np.broadcast_to(\n",
    "#         global_norm[:, np.newaxis, np.newaxis, :],\n",
    "#         (x_data.shape[0], x_data.shape[1], x_data.shape[2], 1)\n",
    "#     )\n",
    "    \n",
    "#     return processed\n",
    "\n",
    "# x_train = preprocess_features(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from keras import backend as K\n",
    "from keras.metrics import Precision, Recall, AUC\n",
    "import time\n",
    "import pickle\n",
    "from keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv1D, Input, Add\n",
    "\n",
    "current_seed = 1028\n",
    "print(current_seed)\n",
    "k = 5\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=1028)\n",
    "histories = []\n",
    "\n",
    "cross_val_results = {\n",
    "    'config': {\n",
    "        'seed': current_seed,\n",
    "        'batch_size': batch_size,\n",
    "        'learning_rate': learning_rate,\n",
    "    },\n",
    "    'folds': []\n",
    "}\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(x_train)):\n",
    "    print(f\"\\nTraining Fold {fold+1}/{k}\")\n",
    "    \n",
    "    K.clear_session()\n",
    "    gc.collect()\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(TimeDistributed(Flatten(), input_shape = x_train.shape[1:]))\n",
    "\n",
    "    model.add(Bidirectional(LSTM(128, return_sequences=True, kernel_regularizer=l2(0.002))))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Bidirectional(LSTM(128, return_sequences=True, kernel_regularizer=l2(0.002))))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.005)))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    optimizer = Adam(learning_rate = learning_rate)\n",
    "    model.compile(optimizer = optimizer, loss = 'binary_crossentropy', metrics = [\n",
    "                      'accuracy', \n",
    "                      Precision(name='precision'), \n",
    "                      Recall(name='recall'),\n",
    "                      AUC(name='auc'),\n",
    "                      F1Score(),\n",
    "                      MatthewsCorrelationCoefficient(name='mcc'), # 添加MCC\n",
    "                      BalancedAccuracy()])\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    reduce_lr = ReduceLROnPlateau(monitor = 'val_loss',\n",
    "                                  factor = reduce_lr_factor,\n",
    "                                  patience = reduce_lr_patience,\n",
    "                                  min_lr = min_learning_rate)\n",
    "    early_stopping = EarlyStopping(monitor = 'val_loss', patience = early_stopping_patience)\n",
    "    \n",
    "    x_val_fold = x_train[val_idx]\n",
    "    y_val_fold = y_train[val_idx]\n",
    "\n",
    "    history = model.fit(\n",
    "                        x_train[train_idx], y_train[train_idx],\n",
    "                        validation_data=(x_val_fold, y_val_fold), \n",
    "                        batch_size = batch_size, \n",
    "                        epochs = epochs,\n",
    "                        callbacks=[early_stopping, reduce_lr])\n",
    "    \n",
    "    best_epoch = np.argmin(history.history['val_loss']) + 1\n",
    "    \n",
    "    y_pred_probs = model.predict(x_val_fold).flatten()\n",
    "    \n",
    "    fold_data = {\n",
    "        'fold_number': fold+1,\n",
    "        'train_indices': train_idx,\n",
    "        'val_indices': val_idx,\n",
    "        'history': history.history,\n",
    "        'best_epoch': best_epoch,\n",
    "        'y_true': y_val_fold,\n",
    "        'y_pred_probs': y_pred_probs,\n",
    "        'final_metrics': dict(zip(\n",
    "            model.metrics_names,\n",
    "            model.evaluate(x_val_fold, y_val_fold, verbose=0)\n",
    "        ))\n",
    "    }\n",
    "    \n",
    "    model.save(f'bilstm_fold_{fold+1}.keras')\n",
    "    with open(f'bilstm_fold_{fold+1}_data.pkl', 'wb') as f:\n",
    "        pickle.dump(fold_data, f)\n",
    "    \n",
    "    cross_val_results['folds'].append(fold_data)\n",
    "\n",
    "    print(\"evaluate result\")\n",
    "    model.evaluate(x_train[val_idx], y_train[val_idx], verbose=0)\n",
    "    with open(f'bilstm_fold_{fold+1}_history.pkl', 'wb') as file_pi:\n",
    "        pickle.dump(history.history, file_pi)\n",
    "        \n",
    "    model.save(r'/bilstm_fold_'+str(fold+1) +'.keras')\n",
    "        \n",
    "    del model\n",
    "    K.clear_session()\n",
    "    gc.collect()\n",
    "    \n",
    "with open('bilstm_cross_validation_results2.pkl', 'wb') as f:\n",
    "    pickle.dump(cross_val_results, f)\n",
    "print(\"\\n所有交叉验证轮次完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
